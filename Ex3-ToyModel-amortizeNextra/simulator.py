import torch
from sbi.utils import BoxUniform
from torch.distributions import Distribution, Categorical, constraints
from torch.distributions.constraints import Constraint

class prior_ToyModel(BoxUniform):
    def __init__(self, low, high):
        super().__init__(low, high)
        self.low = low
        self.high = high

    def condition(self, beta):
        """
        This functions returns the prior distribution just for the alpha
        parameter. It is written like this for compatibility purposes with
        the Pyro framework
        """
        return BoxUniform(low=self.low[:1], high=self.high[:1])



def simulator_ToyModel_amortizeNextra(theta, n_trials=1, p_alpha=None, p_nextra=None, gamma=1.0,
                       sigma=0.0):
    """Define the simulator function
    The model that we consider here is one that gives simply :
    x = alpha * beta^gamma with theta = [alpha, beta]. This model has the
    simplest indeterminacy that one may think of.
    For a given observation xi generated by thetai = [alphai, betai], we may
    also consider a case where we have n_instances other observations with
    different values of alpha but all sharing the same betai.
    Parameters
    ----------
    theta : torchtensor, shape (n_simulations, 2)
        ndarray of simulated theta.
    n_extra : int
        how many extra observations sharing the same beta should we simulate.
        the minimum is 0, for which the output is simply that with theta.
        n_instances > 0 will generate other outputs with other alpha's but the
        same betai. the first coordinate of the sampled observation is the
        one corresponding to the input theta
    p_alpha : torch.distribution
        probability distribution from which to sample the different values of
        alpha for a given betai (only used when n_instances > 1)
    gamma : float
        expoent on the beta parameter
    sigma : float
        standard deviation of the noise
    Returns
    -------
    x : torchtensor shape (n_simulations, n_trials, 1+n_extra)
        observations for the model with different input parameters
    """

    if theta.ndim == 1:
        return simulator_ToyModel_amortizeNextra(theta.view(1, -1), n_trials,
                                  p_alpha, p_nextra, gamma, sigma)

    x = []
    for thetai in theta :
        thetai = thetai.detach().clone()
        alphai = thetai[0]
        betai = thetai[1]
        n_extrai = p_nextra.sample()
        x0_i = torch.tensor([list(alphai * (betai**gamma) + sigma*torch.randn(n_trials))])[0]
        xn_i = torch.tensor([0.])
        
        if n_extrai > 0:
            alphaj_list = p_alpha.condition(betai).sample((n_extrai,))
            betaj = betai
            xn_i = torch.tensor([list(alphaj * (betaj**gamma) + sigma*torch.randn(
                n_trials)) for alphaj in alphaj_list]).mean()
        
        xi = torch.cat([x0_i, torch.tensor([n_extrai]), torch.tensor([xn_i])])
        x.append(xi)
    
    return torch.stack(x)


def get_ground_truth(meta_parameters, p_alpha=None, p_nextra=None):
    "Take the parameters dict as input and output the observed data."

    theta = meta_parameters["theta"].clone()

    observation = simulator_ToyModel_amortizeNextra(theta, 
                                     n_trials=meta_parameters["n_trials"],
                                     p_alpha=p_alpha,
                                     p_nextra=p_nextra,
                                     gamma=meta_parameters["gamma"],
                                     sigma=0.0)

    # get the ground_truth observation data
    ground_truth = {}
    ground_truth["theta"] = meta_parameters["theta"].clone().detach()
    ground_truth["observation"] = observation

    return ground_truth


if __name__ == "__main__":

    from functools import partial

    meta_parameters = {}
    meta_parameters["theta"] = torch.tensor([0.5, 0.5])
    meta_parameters["gt_nextra"] = 10
    meta_parameters["gamma"] = 0
    meta_parameters["noise"] = 0.05
    meta_parameters["nextra_range"] = 40

    nextra_probs = torch.ones(meta_parameters["nextra_range"])/meta_parameters["nextra_range"]
    prior_nextra = Categorical(nextra_probs)

    gt_nextra_prob = torch.zeros_like(nextra_probs)
    gt_nextra_prob[meta_parameters['gt_nextra']] = 1
    p_gt_nextra = Categorical(gt_nextra_prob)

    prior = prior_ToyModel(low=torch.tensor([0.0, 0.0]),
                           high=torch.tensor([1.0, 1.0]))

    meta_parameters["n_trials"] = 1
    ground_truth = get_ground_truth(meta_parameters, prior, p_gt_nextra)
    x = ground_truth['observation']
    print(x)

    theta = prior.sample((100,))

    # choose how to setup the simulator
    simulator = partial(simulator_ToyModel_amortizeNextra,
                        n_trials=meta_parameters["n_trials"],
                        p_alpha=prior,
                        p_nextra=prior_nextra,
                        gamma=meta_parameters["gamma"],
                        sigma=meta_parameters["noise"])

    x = simulator(theta)
    print(x)
